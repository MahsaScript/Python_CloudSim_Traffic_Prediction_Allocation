# -*- coding: utf-8 -*-
"""Python_CloudSim_Network_Traffic_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h44uMDBMUGNAdvjyw93W_NC8Ze__zJPb
"""

!pip install PySDNSim

!sudo apt-get install python dash

!sudo pip install dash==0.29.0

!sudo python -m pip install dash==0.29.0

!pip install jupyter-dash

!pip install --upgrade pip wheel setuptools requests

!python3 -m pip install dash

!pip install jupyter-dash

from collections.abc import MutableMapping

!python3 -m pip install dash

!pip install --upgrade dash dash-core-components dash-html-components dash-renderer

!pip uninstall dash

!pip install dash

#import dash
from dash import Dash, Input, Output, dcc, html

import plotly.express as px

import dash_core_components as dcc
import dash_html_components as html
from dash.dependencies import Input, Output

!pip install treelib

import random
from copy import deepcopy
from typing import List
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
#import jupyter-dash
from dash import Dash, Input, Output, dcc, html
from PySDNSim.Backend import Backend
from PySDNSim.Config import Config
from PySDNSim.Experiment import Experiment
from PySDNSim.Host import Host
from PySDNSim.Job import Job
from PySDNSim.Microservice import Microservice
from PySDNSim.NetworkService import NetworkService
from PySDNSim.NetworkService import create_network_service
from treelib import Node, Tree

"""# create simulation config"""

sim_config = Config(seed=1024, interval=1.0, step_size=0.01)

"""# create host"""

host = Host(
    cpus=64,
    ram=102400,
    bw=100000,
    storage=1024000,
    max_power=1600.0,
    static_power=300.0,
    replicas=3,
)

"""# Create Microservices"""

microservices: List[Microservice] = list()
ms_mqtt_broker = Microservice(
    name="mqtt_broker",
    size=512,
    cpus=2,
    replicas=1,
    max_replicas=3,
    cpu_ratio=25,
    ram_ratio=32,
    bw_ratio=25,
)
ms_mqtt_broker.add_auto_scale("cpu",0.5)
ms_mqtt_broker.add_auto_scale("ram",0.5)
ms_mqtt_broker.add_auto_scale("bw",0.5)
microservices.append(ms_mqtt_broker)
ms_chirpstack_gateway = Microservice(
    name="chirpstack_gateway",
    size=128,
    cpus=2,
    replicas=1,
    max_replicas=3,
    cpu_ratio=25,
    ram_ratio=32,
    bw_ratio=25,
)
ms_chirpstack_gateway.add_auto_scale("cpu",0.5)
ms_chirpstack_gateway.add_auto_scale("ram",0.5)
ms_chirpstack_gateway.add_auto_scale("bw",0.5)
microservices.append(ms_chirpstack_gateway)
ms_chirpstack = Microservice(
    name="chirpstack",
    size=128,
    cpus=4,
    replicas=1,
    max_replicas=3,
    cpu_ratio=10,
    ram_ratio=32,
    bw_ratio=25,
)
ms_chirpstack.add_auto_scale("cpu",0.5)
ms_chirpstack.add_auto_scale("ram",0.5)
ms_chirpstack.add_auto_scale("bw",0.5)
microservices.append(ms_chirpstack)
ms_chirpstack_rest_api = Microservice(
    name="chirpstack_rest_api",
    size=128,
    cpus=2,
    replicas=1,
    max_replicas=3,
    cpu_ratio=5,
    ram_ratio=128,
    bw_ratio=25,
)
ms_chirpstack_rest_api.add_auto_scale("cpu",0.5)
ms_chirpstack_rest_api.add_auto_scale("ram",0.5)
ms_chirpstack_rest_api.add_auto_scale("bw",0.5)
microservices.append(ms_chirpstack_rest_api)
ms_postgresql = Microservice(
    name="postgresql",
    size=2048,
    cpus=2,
    replicas=1,
    max_replicas=3,
    cpu_ratio=50,
    ram_ratio=128,
    bw_ratio=100,
)
ms_postgresql.add_auto_scale("cpu",0.5)
ms_postgresql.add_auto_scale("ram",0.5)
ms_postgresql.add_auto_scale("bw",0.5)
microservices.append(ms_postgresql)
ms_redis = Microservice(
    name="redis",
    size=2048,
    cpus=2,
    replicas=1,
    max_replicas=3,
    cpu_ratio=50,
    ram_ratio=128,
    bw_ratio=100,
)
ms_redis.add_auto_scale("cpu",0.5)
ms_redis.add_auto_scale("ram",0.5)
ms_redis.add_auto_scale("bw",0.5)
microservices.append(ms_redis)

"""# Create network services

"""

ns_list = list()
register_device = create_network_service(
    name="register_device",
    microservices=["chirpstack", "redis", "chirpstack_gateway"],
    schdeule=[0, 1, 1],
    schedule_length=[10, 10, 10],
    ms_pool=microservices
)
ns_list.append(register_device)
read_data = create_network_service(
    name="receive_data",
    microservices=["chirpstack_gateway", "mqtt_broker", "chirpstack", "postgresql"],
    schdeule=[0, 1, 2, 3],
    schedule_length=[10, 10, 10, 10],
    ms_pool=microservices
)
ns_list.append(read_data)
retrive_data = create_network_service(
    name="retrive_data",
    microservices=["chirpstack_rest_api", "chirpstack", "postgresql", "chirpstack"],
    schdeule=[0, 1, 2, 3],
    schedule_length=[10, 10, 10, 10],
    ms_pool=microservices
)
ns_list.append(retrive_data)

"""# Start a simulation that randomly select network services at each iteration."""

from PySDNSim.Backend import Backend

import os
os.system('start backend.jar')

import subprocess
subprocess.call(['java', '-jar', 'backend.jar'])

for iter in range(10):
    num_ns = random.randint(1, 20)
    chosen_ns: List[NetworkService] = deepcopy(random.choices(ns_list, k=num_ns))
    for ns in chosen_ns:
        ns.offset_schedule(random.randint(0, 5))
    experiment = Experiment(
        name=f"{iter}",
        config=sim_config,
        host=host,
        microservices=microservices,
        network_services=chosen_ns,
    )
    bk = Backend()
    bk.run_experiment(experiment=experiment, output_path="results")

    #Backend.run_experiment(self, experiment=experiment, output_path="./results")
    #Backend.run_experiment(self, experiment=experiment, output_path="./results")

import subprocess
a=subprocess.call(
                        [
                            "java",
                            "-jar",
                            "backend.jar",
                            "./configs/0.json",
                            "./results/0.json",
                        ]
                    )

import numpy as np
import pandas as pd

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

data = pd.read_csv("network_traffic_prediction_kdd")
data.head()

# Columns creation and reading of dataset

columns = ["duration", "protocoltype", "service", "flag", "srcbytes", "dstbytes", "land", "wrongfragment",
           "urgent", "hot", "numfailedlogins", "loggedin", "numcompromised", "rootshell", "suattempted",
           "numroot", "numfilecreations", "numshells", "numaccessfiles", "numoutboundcmds", "ishostlogin",
           "isguestlogin", "count", "srvcount", "serrorrate", "srvserrorrate", "rerrorrate", "srvrerrorrate",
           "samesrvrate", "diffsrvrate", "srvdiffhostrate", "dsthostcount", "dsthostsrvcount",
           "dsthostsamesrvrate", "dsthostdiffsrvrate", "dsthostsamesrcportrate", "dsthostsrvdiffhostrate",
           "dsthostserrorrate", "dsthostsrvserrorrate", "dsthostrerrorrate", "dsthostsrvrerror_rate", "labels"]


data = pd.read_csv("network_traffic_prediction_kdd", names=columns)
data.head()

# Data set size
data.shape

# Null values
[col for col in data.columns if data[col].isnull().sum() > 0]

data.dtypes

import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style("darkgrid")
plt.rcParams["figure.figsize"] = (12,8)
font = {"size"   : 11}

plt.rc('font', **font)
grouped_labels = data.groupby("labels")["labels"].count().sort_values(ascending=False)
plt.xticks(rotation=45)
sns.barplot(x=grouped_labels.index, y=grouped_labels.values)
plt.title("Count of attacks and normal events")
plt.ylabel("Count")

grouped_labels = data.groupby("protocoltype")["protocoltype"].count().sort_values(ascending=False)
plt.xticks(rotation=45)
sns.barplot(x=grouped_labels.index, y=grouped_labels.values)
plt.title("labels by Protocol type")
plt.ylabel("Count")

def remove_dot(label):
    """ Remove dot from labels """
    return label.replace(".", "")


data["labels"] = data["labels"].apply(lambda label: remove_dot(label))
print(pd.unique(data["labels"]))

attack_families = {
    "back": "dos",
    "buffer_overflow": "u2r",
    "ftp_write": "r2l",
    "guess_passwd": "r2l",
    "imap": "r2l",
    "ipsweep": "probe",
    "land": "dos",
    "loadmodule": "u2r",
    "multihop": "r2l",
    "neptune": "dos",
    "nmap": "probe",
    "perl": "u2r",
    "phf": "r2l",
    "pod": "dos",
    "portsweep": "probe",
    "rootkit": "u2r",
    "satan": "probe",
    "smurf": "dos",
    "spy": "r2l",
    "teardrop": "dos",
    "warezclient": "r2l",
    "warezmaster": "r2l",
    "normal": "normal",
}


def map_attacks_to_families(attack):
    """ Map attack to it's family """
    return attack_families[attack]


data["labels"] = data["labels"].apply(lambda attack: map_attacks_to_families(attack))
print(pd.unique(data["labels"]))

sns.countplot(x="labels", data=data)
plt.title("Class balance")

sns.catplot(x="protocoltype", y="count", hue="labels", data=data)
plt.title("Number of connections to the same host as the current connection in the past two seconds ")

sns.catplot(x="protocoltype", y="numfilecreations", hue="labels", data=data)

sns.catplot(x="protocoltype", y="numfailedlogins", hue="labels", data=data)

sns.catplot(x="protocoltype", y="numshells", hue="labels", data=data)

sns.catplot(x="protocoltype", y="numaccessfiles", hue="labels", data=data)

sns.catplot(x="protocoltype", y="numroot", hue="labels", data=data)

sns.catplot(x="protocoltype", y="suattempted", hue="labels", data=data)

sns.catplot(x="protocoltype", y="duration", hue="labels", data=data)

sns.catplot(x="protocoltype", y="srcbytes", hue="labels", data=data)

sns.catplot(x="protocoltype", y="dstbytes", hue="labels", data=data)

# Creating feature vector and target vector.
X = data.drop("labels", axis=1)
y = data["labels"]

# Constant features
[col for col in X.columns if X[col].nunique() == 1]

# Those two columns have no variance, thus they won't be of any use to our model. Let's drop them
X.drop(["numoutboundcmds", "ishostlogin"], axis=1, inplace=True)

# Encode categorical columns and split data.
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder


# Encoding target variable
lr = LabelEncoder()
y = lr.fit_transform(y)


# Encoding predictors
enc_protocol = pd.get_dummies(data["protocoltype"], prefix="protocol_")
enc_service = pd.get_dummies(data["service"], prefix="service_")
enc_flag = pd.get_dummies(data["flag"], prefix="flag_")

X = pd.concat([X, enc_protocol, enc_service, enc_flag], axis=1)
X.drop("protocoltype", axis=1, inplace=True)
X.drop("service", axis=1, inplace=True)
X.drop("flag", axis=1, inplace=True)


# Splitting data into train and test...since our data is implaced we stratify it
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y)

# Anova test for feature importance
from sklearn.feature_selection import f_classif

anova_f_classif = f_classif(X_train, y_train)

anova_f_classif = pd.Series(anova_f_classif[1])
anova_f_classif.index = X_train.columns
anova_f_classif.sort_values(ascending=False).plot.bar(figsize=(20, 6))

from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier()

gbc.fit(X_train, y_train)

#GradientBoostingClassifier()

# Prediciton
y_train_pred = gbc.predict(X_train)
y_test_pred = gbc.predict(X_test)

!pip install --upgrade scikit-learn

#from sklearn.metrics import plot_confusion_matrix
#from sklearn.metrics.ConfusionMatrixDisplay import f1_score, plot_confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

from sklearn.metrics import confusion_matrix

from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import f1_score

print("Training f1 score {}".format(f1_score(y_train, y_train_pred, average="macro")))
print("Evaluation f1 score {}".format(f1_score(y_test, y_test_pred, average="macro")))

import sklearn
ConfusionMatrixDisplay.from_estimator(gbc, X_test, y_test)

#(gbc, X_test, y_test)
plt.show()
from sklearn.metrics import classification_report


y_test_transformed = lr.inverse_transform(y_test)
y_predict_transformed = lr.inverse_transform(y_test_pred)
print(classification_report(y_test_transformed, y_predict_transformed))



import xgboost as xgb


xgbc = xgb.XGBRFClassifier()
xgbc.fit(X_train, y_train)
xgbc_pred = xgbc.predict(X_test)
print("Evaluation f1 score {}".format(f1_score(y_test, xgbc_pred, average="macro")))

y_xgb_predict_transformed = lr.inverse_transform(xgbc_pred)

print(classification_report(y_test_transformed, y_xgb_predict_transformed))

ConfusionMatrixDisplay.from_estimator(gbc, X_test, y_test)
plt.show()

from sklearn.metrics import classification_report


y_test_transformed = lr.inverse_transform(y_test)
y_predict_transformed = lr.inverse_transform(y_test_pred)
print(classification_report(y_test_transformed, y_predict_transformed))

import xgboost as xgb


xgbc = xgb.XGBRFClassifier()
xgbc.fit(X_train, y_train)
xgbc_pred = xgbc.predict(X_test)

print("Evaluation f1 score {}".format(f1_score(y_test, xgbc_pred, average="macro")))

y_xgb_predict_transformed = lr.inverse_transform(xgbc_pred)

print(classification_report(y_test_transformed, y_xgb_predict_transformed))