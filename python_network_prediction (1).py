# -*- coding: utf-8 -*-
"""Python_Network_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R4gF5xGAZAlGcE2hO9ZI-ypLxxqgu2-J
"""

import numpy as np
import pandas as pd

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

data = pd.read_csv("network_traffic_prediction_kdd")
data.head()

# Columns creation and reading of dataset

columns = ["duration", "protocoltype", "service", "flag", "srcbytes", "dstbytes", "land", "wrongfragment",
           "urgent", "hot", "numfailedlogins", "loggedin", "numcompromised", "rootshell", "suattempted",
           "numroot", "numfilecreations", "numshells", "numaccessfiles", "numoutboundcmds", "ishostlogin",
           "isguestlogin", "count", "srvcount", "serrorrate", "srvserrorrate", "rerrorrate", "srvrerrorrate",
           "samesrvrate", "diffsrvrate", "srvdiffhostrate", "dsthostcount", "dsthostsrvcount",
           "dsthostsamesrvrate", "dsthostdiffsrvrate", "dsthostsamesrcportrate", "dsthostsrvdiffhostrate",
           "dsthostserrorrate", "dsthostsrvserrorrate", "dsthostrerrorrate", "dsthostsrvrerror_rate", "labels"]


data = pd.read_csv("network_traffic_prediction_kdd", names=columns)
data.head()

# Data set size
data.shape

# Null values
[col for col in data.columns if data[col].isnull().sum() > 0]

data.dtypes

import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style("darkgrid")
plt.rcParams["figure.figsize"] = (12,8)
font = {"size"   : 11}

plt.rc('font', **font)
grouped_labels = data.groupby("labels")["labels"].count().sort_values(ascending=False)
plt.xticks(rotation=45)
sns.barplot(x=grouped_labels.index, y=grouped_labels.values)
plt.title("Count of attacks and normal events")
plt.ylabel("Count")

grouped_labels = data.groupby("protocoltype")["protocoltype"].count().sort_values(ascending=False)
plt.xticks(rotation=45)
sns.barplot(x=grouped_labels.index, y=grouped_labels.values)
plt.title("labels by Protocol type")
plt.ylabel("Count")

def remove_dot(label):
    """ Remove dot from labels """
    return label.replace(".", "")


data["labels"] = data["labels"].apply(lambda label: remove_dot(label))
print(pd.unique(data["labels"]))

attack_families = {
    "back": "dos",
    "buffer_overflow": "u2r",
    "ftp_write": "r2l",
    "guess_passwd": "r2l",
    "imap": "r2l",
    "ipsweep": "probe",
    "land": "dos",
    "loadmodule": "u2r",
    "multihop": "r2l",
    "neptune": "dos",
    "nmap": "probe",
    "perl": "u2r",
    "phf": "r2l",
    "pod": "dos",
    "portsweep": "probe",
    "rootkit": "u2r",
    "satan": "probe",
    "smurf": "dos",
    "spy": "r2l",
    "teardrop": "dos",
    "warezclient": "r2l",
    "warezmaster": "r2l",
    "normal": "normal",
}


def map_attacks_to_families(attack):
    """ Map attack to it's family """
    return attack_families[attack]


data["labels"] = data["labels"].apply(lambda attack: map_attacks_to_families(attack))
print(pd.unique(data["labels"]))

sns.countplot(x="labels", data=data)
plt.title("Class balance")

sns.catplot(x="protocoltype", y="count", hue="labels", data=data)
plt.title("Number of connections to the same host as the current connection in the past two seconds ")

sns.catplot(x="protocoltype", y="numfilecreations", hue="labels", data=data)

sns.catplot(x="protocoltype", y="numfailedlogins", hue="labels", data=data)

sns.catplot(x="protocoltype", y="numshells", hue="labels", data=data)

sns.catplot(x="protocoltype", y="numaccessfiles", hue="labels", data=data)

sns.catplot(x="protocoltype", y="numroot", hue="labels", data=data)

sns.catplot(x="protocoltype", y="suattempted", hue="labels", data=data)

sns.catplot(x="protocoltype", y="duration", hue="labels", data=data)

sns.catplot(x="protocoltype", y="srcbytes", hue="labels", data=data)

sns.catplot(x="protocoltype", y="dstbytes", hue="labels", data=data)

# Creating feature vector and target vector.
X = data.drop("labels", axis=1)
y = data["labels"]

# Constant features
[col for col in X.columns if X[col].nunique() == 1]

# Those two columns have no variance, thus they won't be of any use to our model. Let's drop them
X.drop(["numoutboundcmds", "ishostlogin"], axis=1, inplace=True)

# Encode categorical columns and split data.
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder


# Encoding target variable
lr = LabelEncoder()
y = lr.fit_transform(y)


# Encoding predictors
enc_protocol = pd.get_dummies(data["protocoltype"], prefix="protocol_")
enc_service = pd.get_dummies(data["service"], prefix="service_")
enc_flag = pd.get_dummies(data["flag"], prefix="flag_")

X = pd.concat([X, enc_protocol, enc_service, enc_flag], axis=1)
X.drop("protocoltype", axis=1, inplace=True)
X.drop("service", axis=1, inplace=True)
X.drop("flag", axis=1, inplace=True)


# Splitting data into train and test...since our data is implaced we stratify it
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y)

# Anova test for feature importance
from sklearn.feature_selection import f_classif

anova_f_classif = f_classif(X_train, y_train)

anova_f_classif = pd.Series(anova_f_classif[1])
anova_f_classif.index = X_train.columns
anova_f_classif.sort_values(ascending=False).plot.bar(figsize=(20, 6))

from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier()

gbc.fit(X_train, y_train)

#GradientBoostingClassifier()

# Prediciton
y_train_pred = gbc.predict(X_train)
y_test_pred = gbc.predict(X_test)

!pip install --upgrade scikit-learn

#from sklearn.metrics import plot_confusion_matrix
#from sklearn.metrics.ConfusionMatrixDisplay import f1_score, plot_confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

from sklearn.metrics import confusion_matrix

from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import f1_score

print("Training f1 score {}".format(f1_score(y_train, y_train_pred, average="macro")))
print("Evaluation f1 score {}".format(f1_score(y_test, y_test_pred, average="macro")))

import sklearn
ConfusionMatrixDisplay.from_estimator(gbc, X_test, y_test)

#(gbc, X_test, y_test)
plt.show()
from sklearn.metrics import classification_report


y_test_transformed = lr.inverse_transform(y_test)
y_predict_transformed = lr.inverse_transform(y_test_pred)
print(classification_report(y_test_transformed, y_predict_transformed))



import xgboost as xgb


xgbc = xgb.XGBRFClassifier()
xgbc.fit(X_train, y_train)
xgbc_pred = xgbc.predict(X_test)
print("Evaluation f1 score {}".format(f1_score(y_test, xgbc_pred, average="macro")))

y_xgb_predict_transformed = lr.inverse_transform(xgbc_pred)

print(classification_report(y_test_transformed, y_xgb_predict_transformed))

ConfusionMatrixDisplay.from_estimator(gbc, X_test, y_test)
plt.show()

from sklearn.metrics import classification_report


y_test_transformed = lr.inverse_transform(y_test)
y_predict_transformed = lr.inverse_transform(y_test_pred)
print(classification_report(y_test_transformed, y_predict_transformed))

import xgboost as xgb


xgbc = xgb.XGBRFClassifier()
xgbc.fit(X_train, y_train)
xgbc_pred = xgbc.predict(X_test)

print("Evaluation f1 score {}".format(f1_score(y_test, xgbc_pred, average="macro")))

y_xgb_predict_transformed = lr.inverse_transform(xgbc_pred)

print(classification_report(y_test_transformed, y_xgb_predict_transformed))